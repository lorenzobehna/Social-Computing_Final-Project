padding = TRUE)
# make lower case
tokens_comments <-
tokens_comments |>
tokens_tolower()
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"),
padding = TRUE) |>
tokens_remove(stopwords("en", source = "stopwords-iso")) |>
tokens_remove(stopwords("en", source = "snowball")) |>
tokens_remove(manual_stopwords)
# stemming
tokens_comments <-
tokens_comments |>
tokens_wordstem("english")
# remove empty tokens
tokens_comments <-
tokens_comments |>
tokens_remove("")
# save the token objects
save(tokens_comments, file = "~/projects/Social-Computing_Final-Project/data/comments/R/toksens_comments.RData")
# if needed load the token objects
load("~/projects/Social-Computing_Final-Project/data/comments/R/toksens_comments.RData")
# matching tokens to sentiment dictionary
tokenss_sentiment <-
tokens_lookup(
tokens_comments,
dictionary = data_dictionary_LSD2015[1:2])
head(toks_sentiment, 20)
# matching tokens to sentiment dictionary
tokens_sentiment <-
tokens_lookup(
tokens_comments,
dictionary = data_dictionary_LSD2015[1:2])
head(tokens_sentiment, 20)
# creating dfm out of toks_sentiment
dfm_sentiment_entire_timespan <- dfm(tokens_sentiment)
# converting it back to a data frame with all the docvars
ai_sentiment_entire_timespan <-
cbind(convert(dfm_sentiment_entire_timespan, to = "data.frame"), docvars(dfm_sentiment_entire_timespan)) |>
mutate(pos_to_neg = positive / (positive + negative))
View(ai_sentiment_entire_timespan)
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_point(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 24, length.out=13)),
labels = c(seq(1999, by = 2, length.out=13))) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
dfm_sentiment_graph_entire_timespan
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_point(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 12, length.out=13)),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023)) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_point(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 12, length.out=7)),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023)) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
dfm_sentiment_graph_entire_timespan
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_point(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 12, length.out=8)),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024)) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
dfm_sentiment_graph_entire_timespan
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_scatter(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 12, length.out=8)),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024)) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
dfm_sentiment_graph_entire_timespan
# matching tokens to sentiment dictionary
tokens_sentiment <-
tokens_lookup(
tokens_comments,
dictionary = data_dictionary_LSD2015[1:2])
mydfm_tokens <- dfm(tokens_comments)
mydfm_tokens <- dfm(tokens_comments)
# save the token document feature matrix
save(mydfm_tokens, file = "~/projects/Social-Computing_Final-Project/data/comments/R/mydfm_tokens.RData")
as.matrix(dtm_comments)[1:10,1:10]
#trimming
dtm_comments <- mydfm_tokens |>
dfm_trim(sparsity = 0.999) |>
convert(to = "topicmodels")
as.matrix(dtm_comments)[1:10,1:10]
# remove empty rows
dtm_comments <- dtm_comments[rowSums(as.matrix(dtm_comments)) > 0, ]
# create a topic model pre twitter
lda_model_comments <- LDA(dtm_comments, k = 5)
# creating document term matrix of pre and post twitter data
library(topicmodels)
library(tm)
# create a topic model pre twitter
lda_model_comments <- LDA(dtm_comments, k = 5)
terms(lda_model_comments)
# pull out posterior distribution
posterior_distribution <- posterior(lda_model_comments)
posterior_distribution_term_over_topic <- posterior_distribution$terms
posterior_distribution_topic_over_documents <- posterior_distribution$documents
# pull out top term per topic pre twitter
top_terms_per_topic <-
terms(lda_model_comments, 20)
top_terms_per_topic
library(wordcloud)
# helper function
wordcloud_topic <- function(topics, i) {
cloud.data <- sort(topics[i, ], decreasing = T)[1:100]
wordcloud(names(cloud.data),
freq = cloud.data,
scale = c(4, 0.4),
min.freq = 1,
rot.per = 0,
random.order = F)
}
# printing all the topic word clouds pre twitter
png("wordcloud_top_1.png")
wordcloud_topic(posterior_distribution_term_over_topic, 1)
dev.off()
png("wordcloud_top_2.png")
wordcloud_topic(posterior_distribution_term_over_topic, 2)
dev.off()
png("wordcloud_top_3.png")
wordcloud_topic(posterior_distribution_term_over_topic, 3)
dev.off()
# printing all the topic word clouds pre twitter
png("wordcloud_top_1.png")
wordcloud_topic(posterior_distribution_term_over_topic, 1)
dev.off()
# Set the working directory to where you want to save the files
setwd("~/projects/Social-Computing_Final-Project/data/comments/R/")
# Function to generate and save word clouds for each topic
save_wordcloud <- function(topic_number) {
png_filename <- paste0("wordcloud_top_", topic_number, ".png")
png(png_filename)
wordcloud_topic(posterior_distribution_term_over_topic, topic_number)
dev.off()
}
# Generating and saving word clouds for topics 1, 2, and 3
save_wordcloud(1)
save_wordcloud(2)
save_wordcloud(3)
# remove punctuation
tokens_comments <-
tokens_comments |>
tokens_remove(pattern = "^[[:punct:]]+$",
valuetype = "regex",
padding = TRUE)
# make lower case
tokens_comments <-
tokens_comments |>
tokens_tolower()
# remove stopwords
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"),
padding = TRUE) |>
tokens_remove(stopwords("en", source = "stopwords-iso")) |>
tokens_remove(stopwords("en", source = "snowball")) |>
tokens_remove(manual_stopwords)
# remove stopwords
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"),
padding = TRUE) |>
tokens_remove(stopwords("en", source = "stopwords-iso")) |>
tokens_remove(stopwords("en", source = "snowball"))
# remove stopwords
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"),
padding = TRUE) |>
tokens_remove(stopwords("en", source = "stopwords-iso"))
# remove stopwords
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"), padding = TRUE)
# remove stopwords
tokens_comments <-
tokens_comments |>
tokens_remove(stopwords("english"), padding = TRUE)
# stemming
tokens_comments <-
tokens_comments |>
tokens_wordstem("english")
# remove empty tokens
tokens_comments <-
tokens_comments |>
tokens_remove("")
# save the token objects
save(tokens_comments, file = "~/projects/Social-Computing_Final-Project/data/comments/R/toksens_comments.RData")
# if needed load the token objects
load("~/projects/Social-Computing_Final-Project/data/comments/R/toksens_comments.RData")
mydfm_tokens <- dfm(tokens_comments)
# save the token document feature matrix
save(mydfm_tokens, file = "~/projects/Social-Computing_Final-Project/data/comments/R/mydfm_tokens.RData")
# matching tokens to sentiment dictionary
tokens_sentiment <-
tokens_lookup(
tokens_comments,
dictionary = data_dictionary_LSD2015[1:2])
head(tokens_sentiment, 20)
# creating dfm out of toks_sentiment
dfm_sentiment_entire_timespan <- dfm(tokens_sentiment)
# converting it back to a data frame with all the docvars
ai_sentiment_entire_timespan <-
cbind(convert(dfm_sentiment_entire_timespan, to = "data.frame"), docvars(dfm_sentiment_entire_timespan)) |>
mutate(pos_to_neg = positive / (positive + negative))
# visualising the sentiment over time
dfm_sentiment_graph_entire_timespan <-
ai_sentiment_entire_timespan |>
ggplot(aes(x = comment_running_month,
y = pos_to_neg)) +
labs(title = "Sentiment across all Comments",
subtitle = "Using the LSD2015 Sentiment Dictionary")+
geom_scatter(alpha=0.2) +
scale_x_continuous(name = "Time",
breaks = c(seq(1, by = 12, length.out=8)),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024)) +
scale_y_continuous(name = "Sentiment (0 = negative, 1 = positive)") +
theme_minimal()
dfm_sentiment_graph_entire_timespan
# creating document term matrix of pre and post twitter data
library(topicmodels)
library(tm)
#trimming
dtm_comments <- mydfm_tokens |>
dfm_trim(sparsity = 0.999) |>
convert(to = "topicmodels")
as.matrix(dtm_comments)[1:10,1:10]
# remove empty rows
dtm_comments <- dtm_comments[rowSums(as.matrix(dtm_comments)) > 0, ]
# create a topic model pre twitter
lda_model_comments <- LDA(dtm_comments, k = 5)
terms(lda_model_comments)
# pull out posterior distribution
posterior_distribution <- posterior(lda_model_comments)
posterior_distribution_term_over_topic <- posterior_distribution$terms
posterior_distribution_topic_over_documents <- posterior_distribution$documents
# pull out top topic per document pre twitter
top_topic_per_document_pre <-
topics(lda_model_pre)
top_topic_per_document_pre |> head()
as.matrix(dtm_comments)[1:10,1:10]
# remove empty rows
dtm_comments <- dtm_comments[rowSums(as.matrix(dtm_comments)) > 0, ]
# create a topic model pre twitter
lda_model_comments <- LDA(dtm_comments, k = 5)
terms(lda_model_comments)
# pull out posterior distribution
posterior_distribution <- posterior(lda_model_comments)
posterior_distribution_term_over_topic <- posterior_distribution$terms
posterior_distribution_topic_over_documents <- posterior_distribution$documents
# pull out top topic per document pre twitter
top_topic_per_document <-
topics(lda_model_comments)
top_topic_per_document |> head()
# pull out top term per topic pre twitter
top_terms_per_topic <-
terms(lda_model_comments, 20)
top_terms_per_topic
library(wordcloud)
# helper function
wordcloud_topic <- function(topics, i) {
cloud.data <- sort(topics[i, ], decreasing = T)[1:100]
wordcloud(names(cloud.data),
freq = cloud.data,
scale = c(4, 0.4),
min.freq = 1,
rot.per = 0,
random.order = F)
}
# Set the working directory to where you want to save the files
setwd("~/projects/Social-Computing_Final-Project/data/comments/R/")
# Function to generate and save word clouds for each topic
save_wordcloud <- function(topic_number) {
png_filename <- paste0("wordcloud_top_", topic_number, ".png")
png(png_filename)
wordcloud_topic(posterior_distribution_term_over_topic, topic_number)
dev.off()
}
# Generating and saving word clouds for topics 1, 2, and 3
save_wordcloud(1)
save_wordcloud(2)
save_wordcloud(3)
library(stm)
# creating a document term matrix for the structural topic model
dtm_stm <- mydfm_tokens |>
dfm_trim(sparsity = 0.999) |>
convert(to = "stm")
# number of topics
k = 5
# creating the structural topic model
stm_model <- stm(dtm_stm$documents,
dtm_stm$vocab,
K = k,
data = NULL,
init.type = "LDA")
# label the topics
labelTopics(stm_model)
# create a second structural topic model over timm using comment_running_month
stm_model_2 <- stm(dtm_stm$documents,
dtm_stm$vocab,
K = k,
prevalence = ~ s(comment_running_month ),
data = select(dtm_stm$meta, running_month ),
max.em.its = 1500,
init.type = "Spectral")
# create a second structural topic model over timm using comment_running_month
stm_model_2 <- stm(dtm_stm$documents,
dtm_stm$vocab,
K = k,
prevalence = ~ s(comment_running_month ),
data = select(dtm_stm$meta, comment_running_month ),
max.em.its = 1500,
init.type = "Spectral")
labelTopics(stm_model_2, n = 5)
sl <- sageLabels(stm_model_2, n = 5)
frex <- sl$marginal$frex
l_f1 <- paste0(frex[1,],collapse = " ")
l_f2 <- paste0(frex[2,],collapse = " ")
l_f3 <- paste0(frex[3,],collapse = " ")
l_f4 <- paste0(frex[4,],collapse = " ")
l_f5 <- paste0(frex[5,],collapse = " ")
l_f <- c(l_f1, l_f2, l_f3, l_f4, l_f5)
predicted_probability <- estimateEffect(1:k ~ s(running_month),
stm_model_2,
meta = select(dtm_stm$meta, comment_running_month),
uncertainty = "Global")
predicted_probability <- estimateEffect(1:k ~ s(comment_running_month),
stm_model_2,
meta = select(dtm_stm$meta, comment_running_month),
uncertainty = "Global")
# visualise and print the stm
#png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(10, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 24, length.out = 13),
labels = seq(1999, by = 2, length.out= 13))
}
# visualise and print the stm
#png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(10, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
# visualise and print the stm
#png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(5, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
plot.estimateEffect(predicted_probability,
"comment_running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
# visualise and print the stm
png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(10, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"comment_running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
dev.off()
# visualise and print the stm
png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(10, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"comment_running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "comment_running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
dev.off()
# visualise and print the stm
#png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(10, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"comment_running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "comment_running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
# visualise and print the stm
#png("stm_results.png", height = 1200, width = 600)
par(mfrow = c(5, 1), mar = c(3, 4, 1, .5))
for (i in 1:k) {
plot.estimateEffect(predicted_probability,
"comment_running_month",
method = "continuous",
topics = i,
model = z,
xaxt = "n",
printlegend = FALSE,
xlab = "comment_running_month",
# ylim = c(0, 0.160),
main = paste0("TOPIC ",i,": ",l_f[i] ))
axis(1,
at = seq(1,by = 12, length.out = 8),
labels = c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024))
}
