{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:54:14.122073300Z",
     "start_time": "2024-05-21T15:54:14.110872500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:21.341020700Z",
     "start_time": "2024-05-21T15:56:21.184170100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments = pd.read_pickle(\"../data/comments/all_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:25.565644300Z",
     "start_time": "2024-05-21T15:56:23.171497400Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting strings to datetime\n",
    "df_comments['video_publish_date'] = pd.to_datetime(df_comments['video_publish_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "df_comments['comment_publish_date'] = pd.to_datetime(df_comments['comment_publish_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Extracting year, and month from the datetime column\n",
    "# video\n",
    "df_comments['video_year'] = df_comments['video_publish_date'].dt.year\n",
    "df_comments['video_month'] = df_comments['video_publish_date'].dt.month\n",
    "\n",
    "# comment\n",
    "df_comments['comment_year'] = df_comments['comment_publish_date'].dt.year\n",
    "df_comments['comment_month'] = df_comments['comment_publish_date'].dt.month\n",
    "\n",
    "# define the fixed minimum date\n",
    "min_date = pd.to_datetime('2017-01-01')\n",
    "\n",
    "# Calculate the running month\n",
    "df_comments['video_running_month'] = df_comments['video_month'] + 12 * (df_comments['video_year'] - min_date.year)\n",
    "df_comments['comment_running_month'] = df_comments['comment_month'] + 12 * (df_comments['comment_year'] - min_date.year)\n",
    "\n",
    "# Calculate the running days\n",
    "df_comments['comment_running_days'] = (df_comments['comment_publish_date'] - min_date).dt.days\n",
    "\n",
    "# filter comments after 90 days of videos' release\n",
    "# Calculate the difference in days\n",
    "df_comments['days_publish_date_difference'] = (df_comments['comment_publish_date'] - df_comments['video_publish_date']).dt.days\n",
    "\n",
    "# Filter to include only comments within 90 days of the video publish date\n",
    "df_timely_comments = df_comments[df_comments['days_publish_date_difference'] <= 90]\n",
    "\n",
    "df_timely_comments = df_timely_comments.drop(columns='days_publish_date_difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only timely comments to pickle format\n",
    "df_timely_comments.to_pickle(\"../data/comments/timely_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:31.157317Z",
     "start_time": "2024-05-21T15:56:31.135246400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count words in a string\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Filter out comments with less than 3 words\n",
    "df_timely_comments = df_timely_comments[df_timely_comments['comment_text'].apply(word_count) >= 3]\n",
    "\n",
    "#df_timely_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:57:08.962608500Z",
     "start_time": "2024-05-21T15:57:08.955632500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Text Cleaning Functions\n",
    "\n",
    "# remove all emojis\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, \"\")\n",
    "\n",
    "# Function to normalize text (NOTE: If creating R dataframe, comment out the # Remove punctuations part)\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9_]+\", \" \", text)  # Remove @mentions\n",
    "    text = re.sub(r\"&quot;\", \"\", text) # Remove instances of &quot;\n",
    "    text = re.sub(r\"&#39;\", \"'\", text) # Replace all instances of &#39; with '\n",
    "    text = re.sub(r\"<[^>]*>\", \" \", text) # Remove all HTML tags\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", text)  # Remove URLs\n",
    "    text = re.sub(r\"https?\", \" \", text)  # Remove http/https\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated characters\n",
    "    #text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n",
    "    return text\n",
    "\n",
    "# Function to detect if language is English\n",
    "# ensure consistent results\n",
    "DetectorFactory.seed = 0\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# # Function to correct spelling errors \n",
    "# def correct_spelling(text):\n",
    "#     blob = TextBlob(text)\n",
    "#     corrected_text = blob.correct()\n",
    "#     return str(corrected_text)\n",
    "\n",
    "# collect all the stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     words = text.split()\n",
    "#     # if word is not in stop_words, append it to the list and lower the words\n",
    "#     filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "#     joined_words = ' '.join(filtered_words)\n",
    "    \n",
    "#     return joined_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:57:46.877396100Z",
     "start_time": "2024-05-21T15:57:10.805676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply text cleaning functions (NOTE: If creating R Dataframe comment out the applicaton of remove_stop_words())\n",
    "df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(remove_emojis)\n",
    "df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(normalize_text)\n",
    "\n",
    "#df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(correct_spelling)\n",
    "#df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84       We laugh at Sofia now, but is just a mater of ...\n",
      "103      this is a god begining of my plan to dominate ...\n",
      "114      Do you know where you are Sophia? I'm in a dream.\n",
      "115      this was totaly col.but Sophia was a litle bit...\n",
      "122               At 4:58 why do people think it's a joke.\n",
      "                               ...                        \n",
      "77789    I found the info in this video interesting and...\n",
      "77792    If you're that impresed by the humanoid robot ...\n",
      "77793    While I do apreciate the story here, there are...\n",
      "77796    I wanted to share this experience I had a few ...\n",
      "77797    Thank you for this wonderful video! I have inc...\n",
      "Name: comment_text, Length: 18505, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)  # Set the max column width to unlimited\n",
    "\n",
    "# Apply the is_english() function to the 'comment_text' column\n",
    "df_timely_comments = df_timely_comments[df_timely_comments['comment_text'].apply(is_english)]\n",
    "print(df_timely_comments['comment_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_text_cleaned = df_timely_comments\n",
    "# Save df with cleaned text\n",
    "df_filtered_text_cleaned.to_pickle(\"../data/comments/df-filtered_text-cleaned_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_text_cleaned = pd.read_pickle(\"../data/comments/df-filtered_text-cleaned_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18505 entries, 84 to 77797\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   video_id               18505 non-null  object        \n",
      " 1   video_title            18505 non-null  object        \n",
      " 2   video_publish_date     18505 non-null  datetime64[ns]\n",
      " 3   video_category_id      18505 non-null  object        \n",
      " 4   comment_text           18505 non-null  object        \n",
      " 5   comment_id             18505 non-null  object        \n",
      " 6   comment_publish_date   18505 non-null  datetime64[ns]\n",
      " 7   video_year             18505 non-null  int32         \n",
      " 8   video_month            18505 non-null  int32         \n",
      " 9   comment_year           18505 non-null  int32         \n",
      " 10  comment_month          18505 non-null  int32         \n",
      " 11  video_running_month    18505 non-null  int32         \n",
      " 12  comment_running_month  18505 non-null  int32         \n",
      " 13  comment_running_days   18505 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int32(6), int64(1), object(5)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered_text_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Text with Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:58:49.145873700Z",
     "start_time": "2024-05-21T15:58:49.135053200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define AI-related keywords\n",
    "ai_keywords = [\n",
    "    'artificial intelligence', 'ai', 'a.i.', 'a.i', 'a. i', 'a i.', 'a i', 'machine learning', 'ml', \n",
    "    'deep learning', 'neural networks', 'large language model', 'language model', 'supervised learning', \n",
    "    'unsupervised learning', 'self-driving', 'self driving', 'image recognition', 'speech recognition', \n",
    "    'automation', 'turing test', 'agi', 'artificial general intelligence', 'ani', 'artificial narrow intelligence', \n",
    "    'asi', 'artificial superintelligence', 'algorithm', 'intelligent agent', 'data mining', 'data science', \n",
    "    'computer science', 'intelligent system', 'predictive modeling', 'quantum computing', 'virtual assistant', \n",
    "    'bot', 'robot', 'gpt', 'bard', 'gemini', 'chatgpt', 'transformer', 'openai', 'dalle', 'stable diffusion', \n",
    "    'meta', 'microsoft', 'siri', 'technology', 'terminator', 'skynet', 'prompt', 'copilot'\n",
    "]\n",
    "\n",
    "# Compile regular expressions for all keywords with word boundaries\n",
    "keyword_patterns = [re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE) for keyword in ai_keywords]\n",
    "\n",
    "# Function to check if comment contains any AI-related keywords\n",
    "def contains_keywords(text):\n",
    "    return any(pattern.search(text) for pattern in keyword_patterns)\n",
    "    #return any(keyword.lower() in text.lower() for keyword in ai_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135                      Sophia is the start of terminator\n",
       "444      Y'al didn't believe Trump when he said he woul...\n",
       "472      Asians want human like robots. Europeans want ...\n",
       "509         Wait, which one is the robot? Falon or Sophia?\n",
       "1185     PART 1? THERE WIL BE MORE? YAS 3BLUE1BROWN IS ...\n",
       "                               ...                        \n",
       "77780    I enjoyed this video. The video made me believ...\n",
       "77788    Honestly, it would be very interesting to se a...\n",
       "77789    I found the info in this video interesting and...\n",
       "77792    If you're that impresed by the humanoid robot ...\n",
       "77793    While I do apreciate the story here, there are...\n",
       "Name: comment_text, Length: 6286, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter comments containing AI-related keywords\n",
    "df_filtered_text_keywordfiltered_and_cleaned = df_filtered_text_cleaned[df_filtered_text_cleaned['comment_text'].apply(contains_keywords)] \n",
    "\n",
    "df_filtered_text_keywordfiltered_and_cleaned['comment_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered (1) Dataframe to CSV \n",
    "df_filtered_text_keywordfiltered_and_cleaned.to_csv(\"../data/comments/R/filtered_and_cleaned_comments.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".soco_virtual_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
