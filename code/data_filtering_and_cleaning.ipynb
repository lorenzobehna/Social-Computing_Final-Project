{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:54:14.122073300Z",
     "start_time": "2024-05-21T15:54:14.110872500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:21.341020700Z",
     "start_time": "2024-05-21T15:56:21.184170100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments = pd.read_pickle(\"../data/comments/all_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:25.565644300Z",
     "start_time": "2024-05-21T15:56:23.171497400Z"
    }
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../data/comments/timely_comments.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 32\u001B[0m\n\u001B[0;32m     28\u001B[0m df_timely_comments \u001B[38;5;241m=\u001B[39m df_comments[df_comments[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_publish_date_difference\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m90\u001B[39m]\n\u001B[0;32m     30\u001B[0m df_timely_comments \u001B[38;5;241m=\u001B[39m df_timely_comments\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_publish_date_difference\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 32\u001B[0m \u001B[43mdf_timely_comments\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pickle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/comments/timely_comments.pkl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Universität Zürich UZH\\Desktop\\Uni\\Uni_ZH\\Computerlinguistik\\6_Semester\\Social_Computing\\Project_Git\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    328\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    329\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    330\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    331\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    332\u001B[0m     )\n\u001B[1;32m--> 333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Universität Zürich UZH\\Desktop\\Uni\\Uni_ZH\\Computerlinguistik\\6_Semester\\Social_Computing\\Project_Git\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:3165\u001B[0m, in \u001B[0;36mNDFrame.to_pickle\u001B[1;34m(self, path, compression, protocol, storage_options)\u001B[0m\n\u001B[0;32m   3115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3116\u001B[0m \u001B[38;5;124;03mPickle (serialize) object to file.\u001B[39;00m\n\u001B[0;32m   3117\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3161\u001B[0m \u001B[38;5;124;03m4    4    9\u001B[39;00m\n\u001B[0;32m   3162\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m   3163\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpickle\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_pickle\n\u001B[1;32m-> 3165\u001B[0m \u001B[43mto_pickle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3166\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3167\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3170\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3171\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Universität Zürich UZH\\Desktop\\Uni\\Uni_ZH\\Computerlinguistik\\6_Semester\\Social_Computing\\Project_Git\\venv\\Lib\\site-packages\\pandas\\io\\pickle.py:103\u001B[0m, in \u001B[0;36mto_pickle\u001B[1;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    101\u001B[0m     protocol \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mHIGHEST_PROTOCOL\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001B[39;00m\n\u001B[0;32m    111\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(obj, handles\u001B[38;5;241m.\u001B[39mhandle, protocol\u001B[38;5;241m=\u001B[39mprotocol)\n",
      "File \u001B[1;32m~\\OneDrive - Universität Zürich UZH\\Desktop\\Uni\\Uni_ZH\\Computerlinguistik\\6_Semester\\Social_Computing\\Project_Git\\venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    874\u001B[0m             handle,\n\u001B[0;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    879\u001B[0m         )\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: '../data/comments/timely_comments.pkl'"
     ]
    }
   ],
   "source": [
    "# converting strings to datetime\n",
    "df_comments['video_publish_date'] = pd.to_datetime(df_comments['video_publish_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "df_comments['comment_publish_date'] = pd.to_datetime(df_comments['comment_publish_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Extracting year, and month from the datetime column\n",
    "# video\n",
    "df_comments['video_year'] = df_comments['video_publish_date'].dt.year\n",
    "df_comments['video_month'] = df_comments['video_publish_date'].dt.month\n",
    "\n",
    "# comment\n",
    "df_comments['comment_year'] = df_comments['comment_publish_date'].dt.year\n",
    "df_comments['comment_month'] = df_comments['comment_publish_date'].dt.month\n",
    "\n",
    "# Calculate the running month\n",
    "df_comments['video_running_month'] = df_comments['video_month'] + 12 * (df_comments['video_year'] - df_comments['video_year'].min())\n",
    "df_comments['comment_running_month'] = df_comments['comment_month'] + 12 * (df_comments['comment_year'] - df_comments['comment_year'].min())\n",
    "\n",
    "# define the fixed minimum date\n",
    "min_date = pd.to_datetime('2017-01-01')\n",
    "# Calculate the running days\n",
    "df_comments['comment_running_days'] = (df_comments['comment_publish_date'] - min_date).dt.days\n",
    "\n",
    "# filter comments after 90 days of videos' release\n",
    "# Calculate the difference in days\n",
    "df_comments['days_publish_date_difference'] = (df_comments['comment_publish_date'] - df_comments['video_publish_date']).dt.days\n",
    "\n",
    "# Filter to include only comments within 90 days of the video publish date\n",
    "df_timely_comments = df_comments[df_comments['days_publish_date_difference'] <= 90]\n",
    "\n",
    "df_timely_comments = df_timely_comments.drop(columns='days_publish_date_difference')\n",
    "\n",
    "df_timely_comments.to_pickle(\"../data/comments/timely_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:31.157317Z",
     "start_time": "2024-05-21T15:56:31.135246400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count words in a string\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:42.964841700Z",
     "start_time": "2024-05-21T15:56:42.888964400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out comments with less than 3 words\n",
    "df_timely_comments = df_timely_comments[df_timely_comments['comment_text'].apply(word_count) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:56:44.707991200Z",
     "start_time": "2024-05-21T15:56:44.479359200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19402 entries, 84 to 77797\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   video_id               19402 non-null  object        \n",
      " 1   video_title            19402 non-null  object        \n",
      " 2   video_publish_date     19402 non-null  datetime64[ns]\n",
      " 3   video_category_id      19402 non-null  object        \n",
      " 4   comment_text           19402 non-null  object        \n",
      " 5   comment_id             19402 non-null  object        \n",
      " 6   comment_publish_date   19402 non-null  datetime64[ns]\n",
      " 7   video_year             19402 non-null  int32         \n",
      " 8   video_month            19402 non-null  int32         \n",
      " 9   comment_year           19402 non-null  int32         \n",
      " 10  comment_month          19402 non-null  int32         \n",
      " 11  video_running_month    19402 non-null  int32         \n",
      " 12  comment_running_month  19402 non-null  int32         \n",
      " 13  comment_running_days   19402 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int32(6), int64(1), object(5)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_timely_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:57:08.962608500Z",
     "start_time": "2024-05-21T15:57:08.955632500Z"
    }
   },
   "outputs": [],
   "source": [
    "# text cleaning functions\n",
    "\n",
    "# remove all emojis\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, \"\")\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9_]+\", \" \", text)  # Remove @mentions\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", text)  # Remove URLs\n",
    "    text = re.sub(r\"https?\", \" \", text)  # Remove http/https\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:57:46.877396100Z",
     "start_time": "2024-05-21T15:57:10.805676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishwa\\AppData\\Local\\Temp\\ipykernel_6608\\1577729971.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(remove_emojis).apply(normalize_text)\n"
     ]
    }
   ],
   "source": [
    "# Apply text cleaning functions\n",
    "df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(remove_emojis).apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T15:58:49.145873700Z",
     "start_time": "2024-05-21T15:58:49.135053200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "84       We laugh at Sofia now but is just a mater of t...\n87        quotstay in your lane girlquot strikes again LOL\n103      quotthis is a god begining of my plan to domin...\n114      quotDo you know where you are Sophiaquotbrbrqu...\n115      this was totaly colbut Sophia was a litle bit ...\n                               ...                        \n77789    I found the info in this video interesting and...\n77792    If you39re that impresed by the humanoid robot...\n77793    While I do apreciate the story here there are ...\n77796    I wanted to share this experience I had a few ...\n77797    Thank you for this wonderful video I have incu...\nName: comment_text, Length: 19402, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_timely_comments['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages for removing stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T16:13:04.962531600Z",
     "start_time": "2024-05-21T16:13:03.695052Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'hey, thing like bubble tea'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions to remove stop words\n",
    "\n",
    "# collect all the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    # if word is not in stop_words, append it to the list and lower the words\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "    joined_words = ' '.join(filtered_words)\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "# test on small data\n",
    "#text = 'Hey, there is no such thing like bubble tea'\n",
    "#remove_stop_words(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T16:37:54.555062700Z",
     "start_time": "2024-05-21T16:37:54.518509900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Apply stopword removal\n",
    "df_timely_comments['comment_text'] = df_timely_comments['comment_text'].apply(remove_stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T16:39:32.273275600Z",
     "start_time": "2024-05-21T16:39:32.139998100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "84       laugh sofia mater time tech gets god canot dif...\n87                      quotstay lane girlquot strikes lol\n103      quotthis god begining plan dominate human race...\n114           quotdo know sophiaquotbrbrquoti39m dreamquot\n115              totaly colbut sophia litle bit crepy sasy\n                               ...                        \n77789    found info video interesting frightening fear ...\n77792    you39re impresed humanoid robot featured must ...\n77793    apreciate story asumptions misinterpretations ...\n77796    wanted share experience weks ago invited two n...\n77797    thank wonderful video incured much loses tradi...\nName: comment_text, Length: 19402, dtype: object"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_timely_comments['comment_text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T16:41:16.519564100Z",
     "start_time": "2024-05-21T16:41:16.485246900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".soco_virtual_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
