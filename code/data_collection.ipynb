{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MAX_RESULTS = 30\n",
    "MAX_COMMENTS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Key: Lorenzo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text file containing the API key\n",
    "with open(\"../authentication/YouTube_Data_API_Key_alt1.txt\", \"r\") as file:\n",
    "    API_KEY = file.read().strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Key: Ishwarya**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text file containing the API key\n",
    "with open(\"YOUR PATH TO THE API KEY HERE\", \"r\") as file:\n",
    "    API_KEY = file.read().strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect YouTube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search for videos given a specific query. Requires max_results, published_after, and published_before both in datetime format\n",
    "def search_videos(query, max_results=MAX_RESULTS, published_after=None, published_before=None):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    # convert datetime objects to ISO 8601 string format\n",
    "    published_after_string = published_after.strftime('%Y-%m-%dT%H:%M:%SZ') if published_after else None\n",
    "    published_before_string = published_before.strftime('%Y-%m-%dT%H:%M:%SZ') if published_before else None\n",
    "\n",
    "    ## Two requests are created, to separately search for medium and long lenght videos.\n",
    "    ## This ensures that we don't collect and YouTube Shorts videos, which we are not interested in.\n",
    "\n",
    "    # construct request for medium lenght videos\n",
    "    search_request_medium = youtube.search().list(\n",
    "        q=query, \n",
    "        part=\"snippet\", \n",
    "        type=\"video\", \n",
    "        maxResults=max_results, \n",
    "        order=\"viewCount\",\n",
    "        videoDuration=\"medium\",\n",
    "        relevanceLanguage=\"en\", \n",
    "        publishedAfter=published_after_string, \n",
    "        publishedBefore=published_before_string)\n",
    "    \n",
    "    # construct request for long videos\n",
    "    search_request_long = youtube.search().list(\n",
    "        q=query, \n",
    "        part=\"snippet\", \n",
    "        type=\"video\", \n",
    "        maxResults=max_results, \n",
    "        order=\"viewCount\",\n",
    "        videoDuration=\"long\", \n",
    "        relevanceLanguage=\"en\",\n",
    "        publishedAfter=published_after_string, \n",
    "        publishedBefore=published_before_string)\n",
    "    \n",
    "    # execute both search requests and store the response\n",
    "    search_response_medium = search_request_medium.execute()\n",
    "    search_response_long = search_request_long.execute()\n",
    "\n",
    "    # save video IDs into lists\n",
    "    video_ids_medium = [item['id']['videoId'] for item in search_response_medium['items']]\n",
    "    video_ids_long = [item['id']['videoId'] for item in search_response_long['items']]\n",
    "\n",
    "    # construct request to retrieve data about found videos\n",
    "    video_request_medium = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(video_ids_medium))\n",
    "    video_request_long = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(video_ids_long))\n",
    "\n",
    "    # execute both video requests and store the response\n",
    "    video_response_medium = video_request_medium.execute()\n",
    "    video_response_long = video_request_long.execute()\n",
    "\n",
    "    # create empty list videos_data, in which each list item is a dictionary of metadata about one video\n",
    "    videos_data = []\n",
    "    # loop through the medium length videos and append to videos_data\n",
    "    for item in video_response_medium['items']:\n",
    "        tags_list = item[\"snippet\"].get(\"tags\", []) \n",
    "        videos_data.append({\n",
    "            'video_id': item['id'],\n",
    "            'video_title': item['snippet']['title'],\n",
    "            'channel_title': item['snippet']['channelTitle'],\n",
    "            'channel_id': item['snippet']['channelId'],\n",
    "            'video_publish_date': item['snippet']['publishedAt'],\n",
    "            'view_count': int(item['statistics']['viewCount']) if 'viewCount' in item['statistics'] else 0,\n",
    "            'comment_count': int(item['statistics']['commentCount']) if 'commentCount' in item['statistics'] else 0,\n",
    "            'video_description': item['snippet']['description'],\n",
    "            'tags': tags_list,\n",
    "            'category_id': item['snippet']['categoryId']\n",
    "        })\n",
    "    # loop through the long videos and append to videos_data\n",
    "    for item in video_response_long['items']:\n",
    "        tags_list = item[\"snippet\"].get(\"tags\", []) \n",
    "        videos_data.append({\n",
    "            'video_id': item['id'],\n",
    "            'video_title': item['snippet']['title'],\n",
    "            'channel_title': item['snippet']['channelTitle'],\n",
    "            'channel_id': item['snippet']['channelId'],\n",
    "            'video_publish_date': item['snippet']['publishedAt'],\n",
    "            'view_count': int(item['statistics']['viewCount']) if 'viewCount' in item['statistics'] else 0,\n",
    "            'comment_count': int(item['statistics']['commentCount']) if 'commentCount' in item['statistics'] else 0,\n",
    "            'video_description': item['snippet']['description'],\n",
    "            'tags': tags_list,\n",
    "            'category_id': item['snippet']['categoryId']\n",
    "        })\n",
    "    # convert the list of dictionaries to a dataframe and return\n",
    "    return pd.DataFrame(videos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: Single shot test\n",
    "# # # specify start and end dates\n",
    "# year = 2021\n",
    "\n",
    "# start_date = datetime(year, 1, 1)\n",
    "# end_date = datetime(year, 12, 31)\n",
    "# # call the search videos() function\n",
    "# df_videos = search_videos(\"artificial intelligence\", MAX_RESULTS, start_date, end_date)\n",
    "\n",
    "# # preliminary filtering steps\n",
    "# df_videos = (\n",
    "#     df_videos[df_videos[\"comment_count\"] > 500] # select only videos with at least 500 comments\n",
    "#     .nlargest(20, \"view_count\")  # select the top 10 videos with the highest view_count\n",
    "# )\n",
    "\n",
    "# # save the DataFrame with the appropriate name\n",
    "# df_videos.to_csv(f'../data/videos/df_videos_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2017, 2024):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "\n",
    "    # call the search_videos() function\n",
    "    df_videos = search_videos(\"artificial intelligence\", MAX_RESULTS, start_date, end_date)\n",
    "\n",
    "    # preliminary filtering steps\n",
    "    df_videos = (\n",
    "        df_videos[df_videos[\"comment_count\"] > 500] # select only videos with at least 500 comments\n",
    "        .nlargest(20, \"view_count\")  # select the top 10 videos with the highest view_count\n",
    "    )\n",
    "\n",
    "    # save the DataFrame with the appropriate name\n",
    "    df_videos.to_csv(f'../data/videos/df_videos_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_publish_date</th>\n",
       "      <th>view_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>video_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICUVyDJPzwY</td>\n",
       "      <td>LEGO, but AI controls what I BUILD...</td>\n",
       "      <td>TD BRICKS</td>\n",
       "      <td>UCUU3GdGuQshZFRGnxAPBf_w</td>\n",
       "      <td>2023-04-06T16:48:47Z</td>\n",
       "      <td>21686763</td>\n",
       "      <td>4757</td>\n",
       "      <td>I use ARTIFICIAL INTELLIGENCE to design severa...</td>\n",
       "      <td>[lego, ai, artifical intelligence, lego meme, ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sqa8Zo2XWc4</td>\n",
       "      <td>Artificial Intelligence: Last Week Tonight wit...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>UC3XTzVzaHQEd30rQbuvCtTQ</td>\n",
       "      <td>2023-02-27T07:30:08Z</td>\n",
       "      <td>10262955</td>\n",
       "      <td>11186</td>\n",
       "      <td>Artificial intelligence is increasingly becomi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bk-nQ7HF6k4</td>\n",
       "      <td>EMERGENCY EPISODE: Ex-Google Officer Finally S...</td>\n",
       "      <td>The Diary Of A CEO</td>\n",
       "      <td>UCGq-a57w-aPwyi3pW7XLiHw</td>\n",
       "      <td>2023-06-01T07:00:21Z</td>\n",
       "      <td>9633767</td>\n",
       "      <td>34732</td>\n",
       "      <td>If You Enjoyed This Episode You Must Watch Thi...</td>\n",
       "      <td>[The Diary Of A CEO, steven bartlett steve bar...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>vJefOB8kec8</td>\n",
       "      <td>The Truth about Artificial Intelligence and Ch...</td>\n",
       "      <td>Dhruv Rathee</td>\n",
       "      <td>UC-CSyyi47VX1lD9zyeABW3w</td>\n",
       "      <td>2023-07-16T15:05:29Z</td>\n",
       "      <td>9600532</td>\n",
       "      <td>28000</td>\n",
       "      <td>ü§ñ Join my AI Course: https://academy.dhruvrath...</td>\n",
       "      <td>[Dhruv Rathee, Dhruv, Rathee, indian youtuber,...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uiUPD-z9DTg</td>\n",
       "      <td>The Next Global Superpower Isn't Who You Think...</td>\n",
       "      <td>TED</td>\n",
       "      <td>UCAuUUnT6oDeKwE6v1NGQxug</td>\n",
       "      <td>2023-06-14T16:20:35Z</td>\n",
       "      <td>9209035</td>\n",
       "      <td>13448</td>\n",
       "      <td>Who runs the world? Political scientist Ian Br...</td>\n",
       "      <td>[TEDTalk, TEDTalks, TED Talk, TED Talks, TED, ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jPhJbKBuNnA</td>\n",
       "      <td>I tried using AI. It scared me.</td>\n",
       "      <td>Tom Scott</td>\n",
       "      <td>UCBa659QWEk1AI4Tg--mrJ2A</td>\n",
       "      <td>2023-02-13T16:00:23Z</td>\n",
       "      <td>7379887</td>\n",
       "      <td>14322</td>\n",
       "      <td>I just wanted to fix my email. ‚ñ† AD: üë®‚Äçüíª NordV...</td>\n",
       "      <td>[]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>zaB_20bkoA4</td>\n",
       "      <td>Elon Musk's BRUTALLY Honest Interview With Tuc...</td>\n",
       "      <td>Visionary</td>\n",
       "      <td>UC_8gQrYCIf9aG_9N4_T5fzg</td>\n",
       "      <td>2023-07-06T05:51:15Z</td>\n",
       "      <td>6180406</td>\n",
       "      <td>11401</td>\n",
       "      <td>Elon Musk's BRUTALLY Honest Interview With Tuc...</td>\n",
       "      <td>[Smart Sense, Elon Musk, elon musk carlson tuc...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>aZ5EsdnpLMI</td>\n",
       "      <td>Artificial Intelligence | 60 Minutes Full Epis...</td>\n",
       "      <td>60 Minutes</td>\n",
       "      <td>UCsN32BtMd0IoByjJRNF12cw</td>\n",
       "      <td>2023-12-30T13:00:02Z</td>\n",
       "      <td>5792902</td>\n",
       "      <td>6652</td>\n",
       "      <td>From January 2019, Scott Pelley's interview wi...</td>\n",
       "      <td>[60 minutes, cbs news, artificial intelligence...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DCu9xawHJaw</td>\n",
       "      <td>Meet The 6'10 Ai Robot NBA Players Fear..</td>\n",
       "      <td>REBOUND</td>\n",
       "      <td>UCVepIJedg6BMQeSgjPY4e6A</td>\n",
       "      <td>2023-06-20T19:58:24Z</td>\n",
       "      <td>5232601</td>\n",
       "      <td>1345</td>\n",
       "      <td>If you‚Äôre ever injured in an accident, you can...</td>\n",
       "      <td>[nba, rebound central, rebound]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2yd18z6iSyk</td>\n",
       "      <td>Joe Rogan: \"I Wasn't Afraid of AI Until I Lear...</td>\n",
       "      <td>JRE Daily Clips</td>\n",
       "      <td>UCavUx5z-IziOIxbsK_Ah7ng</td>\n",
       "      <td>2023-12-19T22:46:30Z</td>\n",
       "      <td>5190605</td>\n",
       "      <td>8147</td>\n",
       "      <td>FREE Alpha Brain Trial ‚ñ∫ https://onnit.sjv.io/...</td>\n",
       "      <td>[JRE, UFC, Joe Rogan, Joe Rogan Experience, JR...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>880TBXMuzmk</td>\n",
       "      <td>The AI revolution: Google's developers on the ...</td>\n",
       "      <td>60 Minutes</td>\n",
       "      <td>UCsN32BtMd0IoByjJRNF12cw</td>\n",
       "      <td>2023-04-17T03:13:23Z</td>\n",
       "      <td>4032633</td>\n",
       "      <td>6488</td>\n",
       "      <td>Competitive pressure among tech giants is prop...</td>\n",
       "      <td>[60 Minutes, CBS News, AI, artificial intellig...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kh5dN72GTQ8</td>\n",
       "      <td>What ChatGPT Could Mean for the Future of Arti...</td>\n",
       "      <td>PowerfulJRE</td>\n",
       "      <td>UCzQUP1qoWDoEbmsQxvdjxgQ</td>\n",
       "      <td>2023-01-04T18:48:58Z</td>\n",
       "      <td>4013570</td>\n",
       "      <td>10941</td>\n",
       "      <td>Taken from JRE #1919 w/Bret Weinstein:\\nhttps:...</td>\n",
       "      <td>[Joe Rogan Experience, JRE, Joe, Rogan, podcas...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>j02OZRjf2d8</td>\n",
       "      <td>Chat GPT | ÿßŸÑÿØÿ≠Ÿäÿ≠</td>\n",
       "      <td>New Media Academy Life</td>\n",
       "      <td>UCtJmfVv0a52Bu7BnJJtZXKw</td>\n",
       "      <td>2023-03-07T18:00:08Z</td>\n",
       "      <td>3974387</td>\n",
       "      <td>3293</td>\n",
       "      <td>ÿ±ÿ≠ŸÑÿ© ÿØÿßÿÆŸÑ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑŸÑŸä ÿπÿßÿ±ŸÅ ŸàŸÖÿ¥ ÿπÿßÿ±ŸÅ ...</td>\n",
       "      <td>[chat gpt, how to use chat gpt, chat gpt expla...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ABHz5oZx-WA</td>\n",
       "      <td>I Asked AI about the Second Coming of Jesus, a...</td>\n",
       "      <td>Nick Jones</td>\n",
       "      <td>UCwfeFpvbkp8cgmJWaMO9K0g</td>\n",
       "      <td>2023-06-05T20:33:59Z</td>\n",
       "      <td>3628694</td>\n",
       "      <td>25777</td>\n",
       "      <td>I asked an advanced AI system about the end ti...</td>\n",
       "      <td>[]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>brQLpTnDwyg</td>\n",
       "      <td>Artificial Intelligence Out of Control: The Ap...</td>\n",
       "      <td>The Why Files</td>\n",
       "      <td>UCIFk2uvCNcEmZ77g0ESKLcQ</td>\n",
       "      <td>2023-07-07T01:00:11Z</td>\n",
       "      <td>3615114</td>\n",
       "      <td>18570</td>\n",
       "      <td>Play Conflict of Nations for FREE on PC, iOS, ...</td>\n",
       "      <td>[artificial intelligence, chat gpt, artificial...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jnnj6Fd2ym0</td>\n",
       "      <td>The Future of Artificial Intelligence By Sande...</td>\n",
       "      <td>Sandeep Maheshwari</td>\n",
       "      <td>UCBqFKDipsnzvJdt6UT0lMIg</td>\n",
       "      <td>2023-05-11T06:38:09Z</td>\n",
       "      <td>3400230</td>\n",
       "      <td>6064</td>\n",
       "      <td>Sandeep Maheshwari is a name among millions wh...</td>\n",
       "      <td>[Artificial, Ai, Intelligence, Artificial Inte...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>xoVJKj8lcNQ</td>\n",
       "      <td>The A.I. Dilemma - March 9, 2023</td>\n",
       "      <td>Center for Humane Technology</td>\n",
       "      <td>UCFECM-p3CF81Tp_l2sJsiyg</td>\n",
       "      <td>2023-04-05T19:04:35Z</td>\n",
       "      <td>3399207</td>\n",
       "      <td>5862</td>\n",
       "      <td>Tristan Harris and Aza Raskin discuss how exis...</td>\n",
       "      <td>[]</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>_9LX9HSQkWo</td>\n",
       "      <td>Did We Just Change Animation Forever?</td>\n",
       "      <td>Corridor Crew</td>\n",
       "      <td>UCSpFnDQr88xCZ80N-X7t0nQ</td>\n",
       "      <td>2023-02-26T17:00:31Z</td>\n",
       "      <td>3330350</td>\n",
       "      <td>15434</td>\n",
       "      <td>ANYONE can make a cartoon with this groundbrea...</td>\n",
       "      <td>[anime, animation, machine learning, ai, artif...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rOxO_ICdAmM</td>\n",
       "      <td>I Put Memes Through Artificial Intelligence...</td>\n",
       "      <td>KingSammelot</td>\n",
       "      <td>UCIPKDdl4776Bmw7lJYvZzLg</td>\n",
       "      <td>2023-01-07T19:00:06Z</td>\n",
       "      <td>3168570</td>\n",
       "      <td>6139</td>\n",
       "      <td>WATCH ME LIVE HOLY MOLY - https://twitch.tv/ki...</td>\n",
       "      <td>[geometry dash, gd, robtop, demons, demon, ext...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nnboHTfYsfk</td>\n",
       "      <td>Sky News Australia interviews 'free-thinking' ...</td>\n",
       "      <td>Sky News Australia</td>\n",
       "      <td>UCO0akufu9MOzyz3nvGIXAAw</td>\n",
       "      <td>2023-08-29T10:47:35Z</td>\n",
       "      <td>3164708</td>\n",
       "      <td>11881</td>\n",
       "      <td>Sky News Investigations Reporter Jonathan Lea ...</td>\n",
       "      <td>[6335823593112, business, fb, msn, technology,...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id                                        video_title  \\\n",
       "0   ICUVyDJPzwY              LEGO, but AI controls what I BUILD...   \n",
       "30  Sqa8Zo2XWc4  Artificial Intelligence: Last Week Tonight wit...   \n",
       "31  bk-nQ7HF6k4  EMERGENCY EPISODE: Ex-Google Officer Finally S...   \n",
       "32  vJefOB8kec8  The Truth about Artificial Intelligence and Ch...   \n",
       "1   uiUPD-z9DTg  The Next Global Superpower Isn't Who You Think...   \n",
       "2   jPhJbKBuNnA                    I tried using AI. It scared me.   \n",
       "33  zaB_20bkoA4  Elon Musk's BRUTALLY Honest Interview With Tuc...   \n",
       "34  aZ5EsdnpLMI  Artificial Intelligence | 60 Minutes Full Epis...   \n",
       "3   DCu9xawHJaw          Meet The 6'10 Ai Robot NBA Players Fear..   \n",
       "4   2yd18z6iSyk  Joe Rogan: \"I Wasn't Afraid of AI Until I Lear...   \n",
       "35  880TBXMuzmk  The AI revolution: Google's developers on the ...   \n",
       "5   kh5dN72GTQ8  What ChatGPT Could Mean for the Future of Arti...   \n",
       "36  j02OZRjf2d8                                  Chat GPT | ÿßŸÑÿØÿ≠Ÿäÿ≠   \n",
       "37  ABHz5oZx-WA  I Asked AI about the Second Coming of Jesus, a...   \n",
       "38  brQLpTnDwyg  Artificial Intelligence Out of Control: The Ap...   \n",
       "6   jnnj6Fd2ym0  The Future of Artificial Intelligence By Sande...   \n",
       "39  xoVJKj8lcNQ                   The A.I. Dilemma - March 9, 2023   \n",
       "40  _9LX9HSQkWo              Did We Just Change Animation Forever?   \n",
       "7   rOxO_ICdAmM     I Put Memes Through Artificial Intelligence...   \n",
       "8   nnboHTfYsfk  Sky News Australia interviews 'free-thinking' ...   \n",
       "\n",
       "                   channel_title                channel_id  \\\n",
       "0                      TD BRICKS  UCUU3GdGuQshZFRGnxAPBf_w   \n",
       "30               LastWeekTonight  UC3XTzVzaHQEd30rQbuvCtTQ   \n",
       "31            The Diary Of A CEO  UCGq-a57w-aPwyi3pW7XLiHw   \n",
       "32                  Dhruv Rathee  UC-CSyyi47VX1lD9zyeABW3w   \n",
       "1                            TED  UCAuUUnT6oDeKwE6v1NGQxug   \n",
       "2                      Tom Scott  UCBa659QWEk1AI4Tg--mrJ2A   \n",
       "33                     Visionary  UC_8gQrYCIf9aG_9N4_T5fzg   \n",
       "34                    60 Minutes  UCsN32BtMd0IoByjJRNF12cw   \n",
       "3                        REBOUND  UCVepIJedg6BMQeSgjPY4e6A   \n",
       "4                JRE Daily Clips  UCavUx5z-IziOIxbsK_Ah7ng   \n",
       "35                    60 Minutes  UCsN32BtMd0IoByjJRNF12cw   \n",
       "5                    PowerfulJRE  UCzQUP1qoWDoEbmsQxvdjxgQ   \n",
       "36       New Media Academy Life   UCtJmfVv0a52Bu7BnJJtZXKw   \n",
       "37                    Nick Jones  UCwfeFpvbkp8cgmJWaMO9K0g   \n",
       "38                 The Why Files  UCIFk2uvCNcEmZ77g0ESKLcQ   \n",
       "6             Sandeep Maheshwari  UCBqFKDipsnzvJdt6UT0lMIg   \n",
       "39  Center for Humane Technology  UCFECM-p3CF81Tp_l2sJsiyg   \n",
       "40                 Corridor Crew  UCSpFnDQr88xCZ80N-X7t0nQ   \n",
       "7                   KingSammelot  UCIPKDdl4776Bmw7lJYvZzLg   \n",
       "8             Sky News Australia  UCO0akufu9MOzyz3nvGIXAAw   \n",
       "\n",
       "      video_publish_date  view_count  comment_count  \\\n",
       "0   2023-04-06T16:48:47Z    21686763           4757   \n",
       "30  2023-02-27T07:30:08Z    10262955          11186   \n",
       "31  2023-06-01T07:00:21Z     9633767          34732   \n",
       "32  2023-07-16T15:05:29Z     9600532          28000   \n",
       "1   2023-06-14T16:20:35Z     9209035          13448   \n",
       "2   2023-02-13T16:00:23Z     7379887          14322   \n",
       "33  2023-07-06T05:51:15Z     6180406          11401   \n",
       "34  2023-12-30T13:00:02Z     5792902           6652   \n",
       "3   2023-06-20T19:58:24Z     5232601           1345   \n",
       "4   2023-12-19T22:46:30Z     5190605           8147   \n",
       "35  2023-04-17T03:13:23Z     4032633           6488   \n",
       "5   2023-01-04T18:48:58Z     4013570          10941   \n",
       "36  2023-03-07T18:00:08Z     3974387           3293   \n",
       "37  2023-06-05T20:33:59Z     3628694          25777   \n",
       "38  2023-07-07T01:00:11Z     3615114          18570   \n",
       "6   2023-05-11T06:38:09Z     3400230           6064   \n",
       "39  2023-04-05T19:04:35Z     3399207           5862   \n",
       "40  2023-02-26T17:00:31Z     3330350          15434   \n",
       "7   2023-01-07T19:00:06Z     3168570           6139   \n",
       "8   2023-08-29T10:47:35Z     3164708          11881   \n",
       "\n",
       "                                    video_description  \\\n",
       "0   I use ARTIFICIAL INTELLIGENCE to design severa...   \n",
       "30  Artificial intelligence is increasingly becomi...   \n",
       "31  If You Enjoyed This Episode You Must Watch Thi...   \n",
       "32  ü§ñ Join my AI Course: https://academy.dhruvrath...   \n",
       "1   Who runs the world? Political scientist Ian Br...   \n",
       "2   I just wanted to fix my email. ‚ñ† AD: üë®‚Äçüíª NordV...   \n",
       "33  Elon Musk's BRUTALLY Honest Interview With Tuc...   \n",
       "34  From January 2019, Scott Pelley's interview wi...   \n",
       "3   If you‚Äôre ever injured in an accident, you can...   \n",
       "4   FREE Alpha Brain Trial ‚ñ∫ https://onnit.sjv.io/...   \n",
       "35  Competitive pressure among tech giants is prop...   \n",
       "5   Taken from JRE #1919 w/Bret Weinstein:\\nhttps:...   \n",
       "36  ÿ±ÿ≠ŸÑÿ© ÿØÿßÿÆŸÑ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑŸÑŸä ÿπÿßÿ±ŸÅ ŸàŸÖÿ¥ ÿπÿßÿ±ŸÅ ...   \n",
       "37  I asked an advanced AI system about the end ti...   \n",
       "38  Play Conflict of Nations for FREE on PC, iOS, ...   \n",
       "6   Sandeep Maheshwari is a name among millions wh...   \n",
       "39  Tristan Harris and Aza Raskin discuss how exis...   \n",
       "40  ANYONE can make a cartoon with this groundbrea...   \n",
       "7   WATCH ME LIVE HOLY MOLY - https://twitch.tv/ki...   \n",
       "8   Sky News Investigations Reporter Jonathan Lea ...   \n",
       "\n",
       "                                                 tags category_id  \n",
       "0   [lego, ai, artifical intelligence, lego meme, ...          24  \n",
       "30                                                 []          24  \n",
       "31  [The Diary Of A CEO, steven bartlett steve bar...          22  \n",
       "32  [Dhruv Rathee, Dhruv, Rathee, indian youtuber,...          27  \n",
       "1   [TEDTalk, TEDTalks, TED Talk, TED Talks, TED, ...          25  \n",
       "2                                                  []          27  \n",
       "33  [Smart Sense, Elon Musk, elon musk carlson tuc...          24  \n",
       "34  [60 minutes, cbs news, artificial intelligence...          25  \n",
       "3                     [nba, rebound central, rebound]          17  \n",
       "4   [JRE, UFC, Joe Rogan, Joe Rogan Experience, JR...          23  \n",
       "35  [60 Minutes, CBS News, AI, artificial intellig...          25  \n",
       "5   [Joe Rogan Experience, JRE, Joe, Rogan, podcas...          22  \n",
       "36  [chat gpt, how to use chat gpt, chat gpt expla...          22  \n",
       "37                                                 []          27  \n",
       "38  [artificial intelligence, chat gpt, artificial...          28  \n",
       "6   [Artificial, Ai, Intelligence, Artificial Inte...          22  \n",
       "39                                                 []          29  \n",
       "40  [anime, animation, machine learning, ai, artif...          24  \n",
       "7   [geometry dash, gd, robtop, demons, demon, ext...          20  \n",
       "8   [6335823593112, business, fb, msn, technology,...          25  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the data\n",
    "df_videos.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Videos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NOTE: Probably not yet necessary, we can also do this at a later step.\n",
    "# # Extracting year, month, and day from the datetime column\n",
    "# # df_videos_cleaned['year'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.year\n",
    "# # df_videos_cleaned['month'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.month\n",
    "# # df_videos_cleaned['day'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.day\n",
    "# df_videos_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect YouTube Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reset the comments list again\n",
    "comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets all the top level comments and saves them to a dataframe\n",
    "def get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments=[], next_page_token=None):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    \n",
    "    if comments is None:\n",
    "        comments = []\n",
    "\n",
    "    try:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"id,snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            order=\"relevance\",\n",
    "            pageToken=next_page_token).execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item['snippet']['topLevelComment']\n",
    "            comment_text = comment['snippet']['textDisplay']\n",
    "\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"video_title\": video_title,\n",
    "                \"video_publish_date\": video_publish_date,\n",
    "                \"video_category_id\": video_category_id,\n",
    "                \"comment_text\": comment_text,\n",
    "                \"comment_id\": item['id'],\n",
    "                \"comment_publish_date\": comment[\"snippet\"][\"publishedAt\"]\n",
    "                })\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            return get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments, response[\"nextPageToken\"])\n",
    "        else:\n",
    "            return pd.DataFrame(comments)\n",
    "        \n",
    "    except HttpError as error:\n",
    "        if error.resp.status == 400:\n",
    "            print(f\"HTTP 400 error for video ID {video_id}, with name '{video_title}'. Skipping this page.\")\n",
    "            return pd.DataFrame(comments)  # Return what has been collected so far\n",
    "        elif error.resp.status == 404:\n",
    "            print(f\"HTTP 404 error for video ID {video_id}, with name '{video_title}', date {video_publish_date}, and category id {video_category_id}. Skipping this page.\")\n",
    "            return pd.DataFrame(comments)  # Return what has been collected so far\n",
    "        else:\n",
    "            raise # Re-raise the exception if it's not a 400 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for year 2017:\n",
      "Video IDs\n",
      "['Bg_tJvCA8zw', 'aircAruvnKk', 'WSKi8HfcxEk', 'R9OHn5ZF4Uo', 'S5t6K9iwcdw', 'BrNs0M77Pd4', 'TRzBk_KuIaM', '9kiEK4LrCgQ', 'TlO2gcs1YvM', 'xs_HhZrCBdg']\n",
      "DataFrame for year 2018:\n",
      "Video IDs\n",
      "['Ml9v3wHLuWI', '-cN8sJz50Ng', '1y3XdwTa1cA', 'Ra3fv8gl6NE', 'Pls_q2aQzHg', '-JlxuQ7tPgQ', 'Ktg8E7i4nzw', 'YNLC0wJSHxI', 'gb4SshJ5WOY', '6tBZA2rygcM']\n",
      "DataFrame for year 2019:\n",
      "Video IDs\n",
      "['UwsrzCVZAb8', 'NR32ULxbjYc', '5dZ_lvDgevk', 's0dMTAQM4cw', 'f3lUEnMaiAU', 'JMLsHI8aV0g', 'smK9dgdTl40', 'O5xeyoRL95U', 'Izd2qOgOGQI', '4svUKPeDa5A']\n",
      "DataFrame for year 2020:\n",
      "Video IDs\n",
      "['WXuK6gekU1Y', 'Jky9I1ihAkg', 'XSgfE2vg-Kk', 'WATLfjRHySU', 'tPYj3fFJGjk', '-ePZ7OdY-Dw', 'R69JYEfCSeI', 'R3YFxF0n8n8', '60KJz1BVTyU', '-g0xOJYPjkQ']\n",
      "DataFrame for year 2021:\n",
      "Video IDs\n",
      "['9jkRcrM6XKA', '5q87K1WaoFI', '63yr9dlI0cU', 'fmJ74774RO8', 'aManoLQAHQU', '0yCJMt9Mx9c', 'b8IYaY4aOV0', '5B3Wn6Wo5CU', 'z498dvAYyu0', 'rA5k2S8xPK8']\n",
      "DataFrame for year 2022:\n",
      "Video IDs\n",
      "['GVsUOuSjvcg', 'J6Mdq3n6kgk', 'b2bdGEqPmCI', '0fDJXmqdN-A', 'LzBUm31Vn3k', 's0AqlK_gCbU', 'SVcsDDABEkM', 'mKHkCAdS0gY', 'RzkD_rTEBYs', '17bzlWIGH3g']\n",
      "DataFrame for year 2023:\n",
      "Video IDs\n",
      "['Sqa8Zo2XWc4', 'bk-nQ7HF6k4', 'uiUPD-z9DTg', 'jPhJbKBuNnA', 'aZ5EsdnpLMI', 'DCu9xawHJaw', '2yd18z6iSyk', '880TBXMuzmk', 'kh5dN72GTQ8', 'ABHz5oZx-WA']\n"
     ]
    }
   ],
   "source": [
    "# First we have to read all df_videos.csv files and save them to a list \n",
    "\n",
    "# List to store all the cleaned video dataframes\n",
    "video_dataframes = []\n",
    "\n",
    "# Years for which video CSV files exist\n",
    "years = range(2017, 2023 + 1)\n",
    "\n",
    "# Loop through the years, load the corresponding CSV file, and append it to the list\n",
    "for year in years:\n",
    "    file_path_and_name = f\"../data/videos/cleaned/df_videos_{year}_cleaned.csv\"\n",
    "    df = pd.read_csv(file_path_and_name)\n",
    "    video_dataframes.append(df)\n",
    "\n",
    "# Optional: To verify, print the first few rows of each dataframe\n",
    "for i, df in enumerate(video_dataframes):\n",
    "    print(f\"DataFrame for year {years[i]}:\")\n",
    "    print(\"Video IDs\")\n",
    "    print(df['video_id'].tolist())  # Print all values in the 'video_id' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the dataframe where all comments will be stored\n",
    "df_all_comments = pd.DataFrame(columns=['video_id', 'video_title', 'video_publish_date', 'video_category_id', 'comment_text', 'comment_id', 'comment_publish_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into two halves (for api reasons)\n",
    "half_point = len(video_dataframes) // 2\n",
    "first_half = video_dataframes[:half_point]\n",
    "second_half = video_dataframes[half_point:]\n",
    "\n",
    "for dataframe in first_half:\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # for each row get the 4 relevant columns\n",
    "        video_id = row['video_id']\n",
    "        video_title = row['video_title']\n",
    "        video_publish_date = row['video_publish_date']\n",
    "        video_category_id = row['category_id']\n",
    "\n",
    "        # Call the function and pass the values as parameters\n",
    "        comments = [] # reset the list comments to an empty list, for get_all_top_level_comments() function\n",
    "        df_all_comments_of_one_video = get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments)\n",
    "        df_all_comments =  pd.concat([df_all_comments, df_all_comments_of_one_video], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in second_half:\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # for each row get the 4 relevant columns\n",
    "        video_id = row['video_id']\n",
    "        video_title = row['video_title']\n",
    "        video_publish_date = row['video_publish_date']\n",
    "        video_category_id = row['category_id']\n",
    "\n",
    "        # Call the function and pass the values as parameters\n",
    "        comments = []\n",
    "        df_all_comments_of_one_video = get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments)\n",
    "        df_all_comments =  pd.concat([df_all_comments, df_all_comments_of_one_video], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67904 entries, 0 to 67903\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   video_id              67904 non-null  object\n",
      " 1   video_title           67904 non-null  object\n",
      " 2   video_publish_date    67904 non-null  object\n",
      " 3   video_category_id     67904 non-null  object\n",
      " 4   comment_text          67904 non-null  object\n",
      " 5   comment_id            67904 non-null  object\n",
      " 6   comment_publish_date  67904 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_all_comments.head(50)\n",
    "df_all_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comments.to_pickle(\"../data/comments/all_comments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67904 entries, 0 to 67903\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   video_id              67904 non-null  object\n",
      " 1   video_title           67904 non-null  object\n",
      " 2   video_publish_date    67904 non-null  object\n",
      " 3   video_category_id     67904 non-null  object\n",
      " 4   comment_text          67904 non-null  object\n",
      " 5   comment_id            67904 non-null  object\n",
      " 6   comment_publish_date  67904 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_pickle(\"../data/comments/all_comments.pkl\")\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Collect ALL YouTube comments (including replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_top_level_comment_replies(video_id, video_publish_date, video_title, top_level_comment_id, replies, page_token):\n",
    "#     youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "#     response = youtube.comments().list(\n",
    "#         part=\"snippet\",\n",
    "#         parentId=top_level_comment_id,\n",
    "#         maxResults=100,\n",
    "#         pageToken=page_token).execute()\n",
    "    \n",
    "#     for item in response[\"items\"]:\n",
    "#         replies.append({\n",
    "#             \"video_id\": video_id,\n",
    "#             \"video_title\": video_title,\n",
    "#             \"video_publish_date\": video_publish_date,\n",
    "#             \"text\": item[\"snippet\"][\"textDisplay\"],\n",
    "#             \"comment_published_at\": item[\"snippet\"][\"publishedAt\"],\n",
    "#             \"parent_comment_id\": top_level_comment_id\n",
    "#             })\n",
    "    \n",
    "#     if \"nextPageToken\" in replies:\n",
    "#         return get_all_top_level_comment_replies(top_level_comment_id, replies, response[\"nextPageToken\"])\n",
    "#     else:\n",
    "#         return replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments=[], next_page_token=None):\n",
    "#     youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    \n",
    "#     response = youtube.commentThreads().list(\n",
    "#         part=\"id,snippet,replies\",\n",
    "#         videoId=video_id,\n",
    "#         maxResults=100,\n",
    "#         order=\"relevance\",\n",
    "#         pageToken=next_page_token).execute()\n",
    "\n",
    "#     # Stores the total reply count a top level commnet has.\n",
    "#     #total_reply_count = 0\n",
    "    \n",
    "#     #replies = []\n",
    "\n",
    "#     for item in response[\"items\"]:\n",
    "#         comment = item['snippet']['topLevelComment']\n",
    "#         comment_text = comment['snippet']['textDisplay']\n",
    "\n",
    "#         comments.append({\n",
    "#             \"video_id\": video_id,\n",
    "#             \"video_title\": video_title,\n",
    "#             \"video_publish_date\": video_publish_date,\n",
    "#             \"video_category_id\": video_category_id,\n",
    "#             \"comment_text\": comment_text,\n",
    "#             \"comment_id\": item['id'],\n",
    "#             \"comment_publish_date\": comment[\"snippet\"][\"publishedAt\"]\n",
    "#             })\n",
    "\n",
    "#         # get the total reply count\n",
    "#         # total_reply_count = item['snippet']['totalReplyCount']\n",
    "\n",
    "#         # if (total_reply_count > 0): \n",
    "#         #     replies = []\n",
    "#         #     replies.extend(get_all_top_level_comment_replies(video_id, video_publish_date, video_title, comment['id'], [], None))\n",
    "#         #     comments.extend(replies)\n",
    "    \n",
    "#     if \"nextPageToken\" in response:\n",
    "#         return get_all_comments(video_id, video_title, video_publish_date, video_category_id, comments, response[\"nextPageToken\"])\n",
    "#     else:\n",
    "#         return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = \"mJeNghZXtMo\"\n",
    "# publish_date_str = \"2017-01-30T20:19:44Z\"\n",
    "# title = \"What is Artificial Intelligence\"\n",
    "\n",
    "# publish_date = datetime.strptime(publish_date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# all_comments_df = get_all_top_level_comments(id, title, publish_date, 28)\n",
    "# all_comments_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
