{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MAX_RESULTS = 30\n",
    "MAX_COMMENTS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Key: Lorenzo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text file containing the API key\n",
    "with open(\"../authentication/YouTube_Data_API_Key_alt1.txt\", \"r\") as file:\n",
    "    API_KEY = file.read().strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Key: Ishwarya**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text file containing the API key\n",
    "with open(\"YOUR PATH TO THE API KEY HERE\", \"r\") as file:\n",
    "    API_KEY = file.read().strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect YouTube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search for videos given a specific query. Requires max_results, published_after, and published_before both in datetime format\n",
    "def search_videos(query, max_results=MAX_RESULTS, published_after=None, published_before=None):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    # convert datetime objects to ISO 8601 string format\n",
    "    published_after_string = published_after.strftime('%Y-%m-%dT%H:%M:%SZ') if published_after else None\n",
    "    published_before_string = published_before.strftime('%Y-%m-%dT%H:%M:%SZ') if published_before else None\n",
    "\n",
    "    ## Two requests are created, to separately search for medium and long lenght videos.\n",
    "    ## This ensures that we don't collect and YouTube Shorts videos, which we are not interested in.\n",
    "\n",
    "    # construct request for medium lenght videos\n",
    "    search_request_medium = youtube.search().list(\n",
    "        q=query, \n",
    "        part=\"snippet\", \n",
    "        type=\"video\", \n",
    "        maxResults=max_results, \n",
    "        order=\"viewCount\",\n",
    "        videoDuration=\"medium\",\n",
    "        relevanceLanguage=\"en\", \n",
    "        publishedAfter=published_after_string, \n",
    "        publishedBefore=published_before_string)\n",
    "    \n",
    "    # construct request for long videos\n",
    "    search_request_long = youtube.search().list(\n",
    "        q=query, \n",
    "        part=\"snippet\", \n",
    "        type=\"video\", \n",
    "        maxResults=max_results, \n",
    "        order=\"viewCount\",\n",
    "        videoDuration=\"long\", \n",
    "        relevanceLanguage=\"en\",\n",
    "        publishedAfter=published_after_string, \n",
    "        publishedBefore=published_before_string)\n",
    "    \n",
    "    # execute both search requests and store the response\n",
    "    search_response_medium = search_request_medium.execute()\n",
    "    search_response_long = search_request_long.execute()\n",
    "\n",
    "    # save video IDs into lists\n",
    "    video_ids_medium = [item['id']['videoId'] for item in search_response_medium['items']]\n",
    "    video_ids_long = [item['id']['videoId'] for item in search_response_long['items']]\n",
    "\n",
    "    # construct request to retrieve data about found videos\n",
    "    video_request_medium = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(video_ids_medium))\n",
    "    video_request_long = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(video_ids_long))\n",
    "\n",
    "    # execute both video requests and store the response\n",
    "    video_response_medium = video_request_medium.execute()\n",
    "    video_response_long = video_request_long.execute()\n",
    "\n",
    "    # create empty list videos_data, in which each list item is a dictionary of metadata about one video\n",
    "    videos_data = []\n",
    "    # loop through the medium length videos and append to videos_data\n",
    "    for item in video_response_medium['items']:\n",
    "        tags_list = item[\"snippet\"].get(\"tags\", []) \n",
    "        videos_data.append({\n",
    "            'video_id': item['id'],\n",
    "            'video_title': item['snippet']['title'],\n",
    "            'channel_title': item['snippet']['channelTitle'],\n",
    "            'channel_id': item['snippet']['channelId'],\n",
    "            'video_publish_date': item['snippet']['publishedAt'],\n",
    "            'view_count': int(item['statistics']['viewCount']) if 'viewCount' in item['statistics'] else 0,\n",
    "            'comment_count': int(item['statistics']['commentCount']) if 'commentCount' in item['statistics'] else 0,\n",
    "            'video_description': item['snippet']['description'],\n",
    "            'tags': tags_list,\n",
    "            'category_id': item['snippet']['categoryId']\n",
    "        })\n",
    "    # loop through the long videos and append to videos_data\n",
    "    for item in video_response_long['items']:\n",
    "        tags_list = item[\"snippet\"].get(\"tags\", []) \n",
    "        videos_data.append({\n",
    "            'video_id': item['id'],\n",
    "            'video_title': item['snippet']['title'],\n",
    "            'channel_title': item['snippet']['channelTitle'],\n",
    "            'channel_id': item['snippet']['channelId'],\n",
    "            'video_publish_date': item['snippet']['publishedAt'],\n",
    "            'view_count': int(item['statistics']['viewCount']) if 'viewCount' in item['statistics'] else 0,\n",
    "            'comment_count': int(item['statistics']['commentCount']) if 'commentCount' in item['statistics'] else 0,\n",
    "            'video_description': item['snippet']['description'],\n",
    "            'tags': tags_list,\n",
    "            'category_id': item['snippet']['categoryId']\n",
    "        })\n",
    "    # convert the list of dictionaries to a dataframe and return\n",
    "    return pd.DataFrame(videos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: Single shot test\n",
    "# # # specify start and end dates\n",
    "# year = 2021\n",
    "\n",
    "# start_date = datetime(year, 1, 1)\n",
    "# end_date = datetime(year, 12, 31)\n",
    "# # call the search videos() function\n",
    "# df_videos = search_videos(\"artificial intelligence\", MAX_RESULTS, start_date, end_date)\n",
    "\n",
    "# # preliminary filtering steps\n",
    "# df_videos = (\n",
    "#     df_videos[df_videos[\"comment_count\"] > 500] # select only videos with at least 500 comments\n",
    "#     .nlargest(20, \"view_count\")  # select the top 10 videos with the highest view_count\n",
    "# )\n",
    "\n",
    "# # save the DataFrame with the appropriate name\n",
    "# df_videos.to_csv(f'../data/videos/df_videos_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the videos dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2017, 2024):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "\n",
    "    # call the search_videos() function\n",
    "    df_videos = search_videos(\"artificial intelligence\", MAX_RESULTS, start_date, end_date)\n",
    "\n",
    "    # preliminary filtering steps\n",
    "    df_videos = (\n",
    "        df_videos[df_videos[\"comment_count\"] > 500] # select only videos with at least 500 comments\n",
    "        .nlargest(20, \"view_count\")  # select the top 10 videos with the highest view_count\n",
    "    )\n",
    "\n",
    "    # save the DataFrame with the appropriate name\n",
    "    df_videos.to_csv(f'../data/videos/df_videos_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "#df_videos.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Videos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NOTE: Probably not yet necessary, we can also do this at a later step.\n",
    "# # Extracting year, month, and day from the datetime column\n",
    "# # df_videos_cleaned['year'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.year\n",
    "# # df_videos_cleaned['month'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.month\n",
    "# # df_videos_cleaned['day'] = pd.to_datetime(df_videos_cleaned['publish_date']).dt.day\n",
    "# df_videos_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect YouTube Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets all the top level comments and saves them to a dataframe\n",
    "def get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments=[], next_page_token=None):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    \n",
    "    if comments is None:\n",
    "        comments = []\n",
    "\n",
    "    try:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"id,snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            order=\"relevance\",\n",
    "            pageToken=next_page_token).execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item['snippet']['topLevelComment']\n",
    "            comment_text = comment['snippet']['textDisplay']\n",
    "\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"video_title\": video_title,\n",
    "                \"video_publish_date\": video_publish_date,\n",
    "                \"video_category_id\": video_category_id,\n",
    "                \"comment_text\": comment_text,\n",
    "                \"comment_id\": item['id'],\n",
    "                \"comment_publish_date\": comment[\"snippet\"][\"publishedAt\"]\n",
    "                })\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            return get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments, response[\"nextPageToken\"])\n",
    "        else:\n",
    "            return pd.DataFrame(comments)\n",
    "        \n",
    "    except HttpError as error:\n",
    "        if error.resp.status == 400:\n",
    "            print(f\"HTTP 400 error for video ID {video_id}, with name '{video_title}'. Skipping this page.\")\n",
    "            return pd.DataFrame(comments)  # Return what has been collected so far\n",
    "        elif error.resp.status == 404:\n",
    "            print(f\"HTTP 404 error for video ID {video_id}, with name '{video_title}', date {video_publish_date}, and category id {video_category_id}. Skipping this page.\")\n",
    "            return pd.DataFrame(comments)  # Return what has been collected so far\n",
    "        else:\n",
    "            raise # Re-raise the exception if it's not a 400 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading in the videos dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for year 2017:\n",
      "Video IDs\n",
      "['Bg_tJvCA8zw', 'aircAruvnKk', 'WSKi8HfcxEk', 'R9OHn5ZF4Uo', 'S5t6K9iwcdw', 'BrNs0M77Pd4', 'TRzBk_KuIaM', '9kiEK4LrCgQ', 'TlO2gcs1YvM', 'xs_HhZrCBdg']\n",
      "DataFrame for year 2018:\n",
      "Video IDs\n",
      "['Ml9v3wHLuWI', '-cN8sJz50Ng', '1y3XdwTa1cA', 'Ra3fv8gl6NE', 'Pls_q2aQzHg', '-JlxuQ7tPgQ', 'Ktg8E7i4nzw', 'YNLC0wJSHxI', 'gb4SshJ5WOY', '6tBZA2rygcM']\n",
      "DataFrame for year 2019:\n",
      "Video IDs\n",
      "['UwsrzCVZAb8', 'NR32ULxbjYc', '5dZ_lvDgevk', 's0dMTAQM4cw', 'f3lUEnMaiAU', 'JMLsHI8aV0g', 'smK9dgdTl40', 'O5xeyoRL95U', 'Izd2qOgOGQI', '4svUKPeDa5A']\n",
      "DataFrame for year 2020:\n",
      "Video IDs\n",
      "['WXuK6gekU1Y', 'Jky9I1ihAkg', 'XSgfE2vg-Kk', 'WATLfjRHySU', 'tPYj3fFJGjk', '-ePZ7OdY-Dw', 'R69JYEfCSeI', 'R3YFxF0n8n8', '60KJz1BVTyU', '-g0xOJYPjkQ']\n",
      "DataFrame for year 2021:\n",
      "Video IDs\n",
      "['9jkRcrM6XKA', '5q87K1WaoFI', '63yr9dlI0cU', 'fmJ74774RO8', 'aManoLQAHQU', '0yCJMt9Mx9c', 'b8IYaY4aOV0', '5B3Wn6Wo5CU', 'z498dvAYyu0', 'rA5k2S8xPK8']\n",
      "DataFrame for year 2022:\n",
      "Video IDs\n",
      "['GVsUOuSjvcg', 'J6Mdq3n6kgk', 'b2bdGEqPmCI', '0fDJXmqdN-A', 'LzBUm31Vn3k', 's0AqlK_gCbU', 'SVcsDDABEkM', 'mKHkCAdS0gY', 'RzkD_rTEBYs', '17bzlWIGH3g']\n",
      "DataFrame for year 2023:\n",
      "Video IDs\n",
      "['Sqa8Zo2XWc4', 'bk-nQ7HF6k4', 'uiUPD-z9DTg', 'jPhJbKBuNnA', 'aZ5EsdnpLMI', 'DCu9xawHJaw', '2yd18z6iSyk', '880TBXMuzmk', 'kh5dN72GTQ8', 'ABHz5oZx-WA']\n"
     ]
    }
   ],
   "source": [
    "# First we have to read all df_videos.csv files and save them to a list \n",
    "\n",
    "# List to store all the cleaned video dataframes\n",
    "video_dataframes = []\n",
    "\n",
    "# Years for which video CSV files exist\n",
    "years = range(2017, 2023 + 1)\n",
    "\n",
    "# Loop through the years, load the corresponding CSV file, and append it to the list\n",
    "for year in years:\n",
    "    file_path_and_name = f\"../data/videos/cleaned/df_videos_{year}_cleaned.csv\"\n",
    "    df = pd.read_csv(file_path_and_name)\n",
    "    video_dataframes.append(df)\n",
    "\n",
    "# Optional: To verify, print the first few rows of each dataframe\n",
    "for i, df in enumerate(video_dataframes):\n",
    "    print(f\"DataFrame for year {years[i]}:\")\n",
    "    print(\"Video IDs\")\n",
    "    print(df['video_id'].tolist())  # Print all values in the 'video_id' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the dataframe where all comments will be stored\n",
    "df_all_comments = pd.DataFrame(columns=['video_id', 'video_title', 'video_publish_date', 'video_category_id', 'comment_text', 'comment_id', 'comment_publish_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into two halves (for api reasons)\n",
    "half_point = len(video_dataframes) // 2\n",
    "first_half = video_dataframes[:half_point]\n",
    "second_half = video_dataframes[half_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in first_half:\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # for each row get the 4 relevant columns\n",
    "        video_id = row['video_id']\n",
    "        video_title = row['video_title']\n",
    "        video_publish_date = row['video_publish_date']\n",
    "        video_category_id = row['category_id']\n",
    "\n",
    "        # Call the function and pass the values as parameters\n",
    "        comments = [] # reset the list comments to an empty list, for get_all_top_level_comments() function\n",
    "        df_all_comments_of_one_video = get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments)\n",
    "        df_all_comments =  pd.concat([df_all_comments, df_all_comments_of_one_video], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in second_half:\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # for each row get the 4 relevant columns\n",
    "        video_id = row['video_id']\n",
    "        video_title = row['video_title']\n",
    "        video_publish_date = row['video_publish_date']\n",
    "        video_category_id = row['category_id']\n",
    "\n",
    "        # Call the function and pass the values as parameters\n",
    "        comments = []\n",
    "        df_all_comments_of_one_video = get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments)\n",
    "        df_all_comments =  pd.concat([df_all_comments, df_all_comments_of_one_video], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in video_dataframes:\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # for each row get the 4 relevant columns\n",
    "        video_id = row['video_id']\n",
    "        video_title = row['video_title']\n",
    "        video_publish_date = row['video_publish_date']\n",
    "        video_category_id = row['category_id']\n",
    "\n",
    "        # Call the function and pass the values as parameters\n",
    "        comments = []\n",
    "        df_all_comments_of_one_video = get_all_top_level_comments(video_id, video_title, video_publish_date, video_category_id, comments)\n",
    "        df_all_comments =  pd.concat([df_all_comments, df_all_comments_of_one_video], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataframe of all comments\n",
    "df_all_comments.to_pickle(\"../data/comments/all_comments.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
